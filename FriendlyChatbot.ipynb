{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5MybE3op33sbCuFkZOdpt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Kbyn4TZ3lrPM"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch sentencepiece accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU d\u00e9tect\u00e9: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   M\u00e9moire disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Pas de GPU d\u00e9tect\u00e9. Le chatbot fonctionnera sur CPU (plus lent)\")\n",
        "    print(\"   Pour activer le GPU: Runtime \u2192 Change runtime type \u2192 GPU\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fObEsZ4Slta7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763507824501,
          "user_tz": -60,
          "elapsed": 6054,
          "user": {
            "displayName": "Ayhem Chaabane",
            "userId": "02755421382646347941"
          }
        },
        "outputId": "f90de1f5-ca32-4474-e9bd-73cc691eccd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\ude80 GPU d\u00e9tect\u00e9: Tesla T4\n",
            "   M\u00e9moire disponible: 15.83 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BlenderbotTokenizer,\n",
        "    BlenderbotForConditionalGeneration,\n",
        "    pipeline\n",
        ")\n",
        "import torch\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import time\n"
      ],
      "metadata": {
        "id": "mk7aYo-2l1td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BlenderBotPoli:\n",
        "    \"\"\"\n",
        "    BlenderBot est BEAUCOUP plus intelligent et poli que DialoGPT\n",
        "    Entra\u00een\u00e9 sp\u00e9cifiquement pour \u00eatre empathique et engageant\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"facebook/blenderbot-400M-distill\"):\n",
        "        print(f\"\ud83d\udce5 Chargement de BlenderBot (plus intelligent et poli)...\")\n",
        "        start = time.time()\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
        "        self.model = BlenderbotForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"\u2705 BlenderBot charg\u00e9 en {elapsed:.2f}s!\\n\")\n",
        "\n",
        "    def generer_reponse(self, texte_utilisateur):\n",
        "        \"\"\"G\u00e9n\u00e9rer une r\u00e9ponse intelligente et polie\"\"\"\n",
        "        inputs = self.tokenizer([texte_utilisateur], return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        reply_ids = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=150,  # R\u00e9ponses plus longues\n",
        "            min_length=20,   # Minimum de longueur\n",
        "            num_beams=8,     # Plus de beams = meilleure qualit\u00e9\n",
        "            length_penalty=0.6,  # Favorise les r\u00e9ponses plus longues\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "        reponse = self.tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
        "        return reponse"
      ],
      "metadata": {
        "id": "ooraGpAymGc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DialoGPTAmeliore:\n",
        "    \"\"\"\n",
        "    DialoGPT am\u00e9lior\u00e9 avec filtre de politesse\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
        "        print(f\"\ud83d\udce5 Chargement de DialoGPT am\u00e9lior\u00e9...\")\n",
        "        start = time.time()\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
        "        self.chat_history_ids = None\n",
        "\n",
        "        # Ajouter un contexte de personnalit\u00e9 polie\n",
        "        self.system_context = \"I am a helpful, respectful, and friendly assistant.\"\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"\u2705 Mod\u00e8le charg\u00e9 en {elapsed:.2f}s!\\n\")\n",
        "\n",
        "    def filtrer_reponse(self, reponse):\n",
        "        \"\"\"Filtrer les r\u00e9ponses impolies ou inappropri\u00e9es\"\"\"\n",
        "        mots_impolis = ['stupid', 'idiot', 'shut up', 'whatever', 'boring', 'dumb']\n",
        "        reponse_lower = reponse.lower()\n",
        "\n",
        "        # Si la r\u00e9ponse contient des mots impolis, g\u00e9n\u00e9rer une alternative\n",
        "        if any(mot in reponse_lower for mot in mots_impolis):\n",
        "            return None\n",
        "\n",
        "        # Si la r\u00e9ponse est trop courte (moins de 10 caract\u00e8res)\n",
        "        if len(reponse.strip()) < 10:\n",
        "            return None\n",
        "\n",
        "        return reponse\n",
        "\n",
        "    def generer_reponse(self, texte_utilisateur, max_attempts=3):\n",
        "        \"\"\"G\u00e9n\u00e9rer une r\u00e9ponse avec filtrage\"\"\"\n",
        "        for attempt in range(max_attempts):\n",
        "            # Encoder avec contexte\n",
        "            new_input_ids = self.tokenizer.encode(\n",
        "                texte_utilisateur + self.tokenizer.eos_token,\n",
        "                return_tensors='pt'\n",
        "            ).to(self.device)\n",
        "\n",
        "            if self.chat_history_ids is not None:\n",
        "                bot_input_ids = torch.cat([self.chat_history_ids, new_input_ids], dim=-1)\n",
        "            else:\n",
        "                bot_input_ids = new_input_ids\n",
        "\n",
        "            # G\u00e9n\u00e9rer avec param\u00e8tres ajust\u00e9s pour plus de politesse\n",
        "            self.chat_history_ids = self.model.generate(\n",
        "                bot_input_ids,\n",
        "                max_length=1200,\n",
        "                min_length=bot_input_ids.shape[-1] + 20,  # Minimum de longueur\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                temperature=0.7,  # Moins al\u00e9atoire\n",
        "                top_k=50,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.2,  # \u00c9viter les r\u00e9p\u00e9titions\n",
        "                no_repeat_ngram_size=3,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            reponse = self.tokenizer.decode(\n",
        "                self.chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            # Filtrer la r\u00e9ponse\n",
        "            reponse_filtree = self.filtrer_reponse(reponse)\n",
        "            if reponse_filtree:\n",
        "                return reponse_filtree\n",
        "\n",
        "        # Si toutes les tentatives \u00e9chouent, retourner une r\u00e9ponse par d\u00e9faut\n",
        "        return \"I'd be happy to help! Could you tell me more about what you're interested in?\"\n",
        "\n",
        "    def reinitialiser(self):\n",
        "        self.chat_history_ids = None"
      ],
      "metadata": {
        "id": "5Ti_6LM8mKKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatbotAvance:\n",
        "    \"\"\"\n",
        "    Utilise des mod\u00e8les plus r\u00e9cents et intelligents\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"gpt2-medium\"):\n",
        "        \"\"\"\n",
        "        Mod\u00e8les disponibles:\n",
        "        - gpt2-medium (355M params)\n",
        "        - gpt2-large (774M params)\n",
        "        - distilgpt2 (plus rapide)\n",
        "        \"\"\"\n",
        "        print(f\"\ud83d\udce5 Chargement de {model_name}...\")\n",
        "        start = time.time()\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "        # Syst\u00e8me prompt pour rendre le bot poli\n",
        "        self.system_prompt = \"\"\"You are a helpful, respectful, and polite AI assistant.\n",
        "You provide thoughtful, detailed, and kind responses to users' questions.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"\u2705 {model_name} charg\u00e9 en {elapsed:.2f}s!\\n\")\n",
        "\n",
        "    def generer_reponse(self, texte_utilisateur):\n",
        "        \"\"\"G\u00e9n\u00e9rer une r\u00e9ponse avec syst\u00e8me prompt\"\"\"\n",
        "        # Cr\u00e9er le prompt complet\n",
        "        prompt = f\"{self.system_prompt}User: {texte_utilisateur}\\nAssistant:\"\n",
        "\n",
        "        # Encoder\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # G\u00e9n\u00e9rer\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=inputs['input_ids'].shape[1] + 100,\n",
        "            min_length=inputs['input_ids'].shape[1] + 30,\n",
        "            temperature=0.8,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.3,\n",
        "            no_repeat_ngram_size=3,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # D\u00e9coder et extraire seulement la r\u00e9ponse\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = full_response.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "        # Nettoyer la r\u00e9ponse\n",
        "        if \"User:\" in response:\n",
        "            response = response.split(\"User:\")[0].strip()\n",
        "\n",
        "        return response\n"
      ],
      "metadata": {
        "id": "nqbEMe0XuQnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\ud83c\udfaf CHOISISSEZ VOTRE MOD\u00c8LE:\\n\")\n",
        "print(\"1\ufe0f\u20e3  BlenderBot - Le PLUS POLI et intelligent (RECOMMAND\u00c9) \u2b50\u2b50\u2b50\u2b50\u2b50\")\n",
        "print(\"2\ufe0f\u20e3  DialoGPT Am\u00e9lior\u00e9 - Avec filtres de politesse \u2b50\u2b50\u2b50\")\n",
        "print(\"3\ufe0f\u20e3  GPT-2 Medium - Intelligent mais besoin de prompts \u2b50\u2b50\u2b50\u2b50\")\n",
        "print()\n",
        "\n",
        "# CHANGEZ CE NUM\u00c9RO POUR CHOISIR VOTRE MOD\u00c8LE\n",
        "choix = \"1\"\n",
        "\n",
        "if choix == \"1\":\n",
        "    bot = BlenderBotPoli()\n",
        "    print(\"\u2705 BlenderBot activ\u00e9 - Mode POLI et INTELLIGENT!\")\n",
        "elif choix == \"2\":\n",
        "    bot = DialoGPTAmeliore()\n",
        "    print(\"\u2705 DialoGPT Am\u00e9lior\u00e9 activ\u00e9!\")\n",
        "elif choix == \"3\":\n",
        "    bot = ChatbotAvance(\"gpt2-medium\")\n",
        "    print(\"\u2705 GPT-2 Medium activ\u00e9!\")\n",
        "else:\n",
        "    bot = BlenderBotPoli()\n",
        "    print(\"\u2705 BlenderBot activ\u00e9 par d\u00e9faut!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83d\udcac Chatbot pr\u00eat - Il sera beaucoup plus poli maintenant!\")\n",
        "print(\"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758,
          "referenced_widgets": [
            "7b90af2e034e46ab8420d9b9f767d35b",
            "db95cf32cc2b421fa77bd7dfe4c2c9ad",
            "e89e1aef345d4a14bd23574719338449",
            "e977c0645b904e538ace5cea0caa1c64",
            "9a47fcaf6d5e4c428fccfa29ac8f78b4",
            "a6bd4e118f6b43c4968991609ff3ddee",
            "9b2960e47c6b41bd9c7dcf56f0df0d36",
            "23fd5529ebc94de1b7460f37ff4ad682",
            "ddfc3a8e3f964dca806280a0998f4af0",
            "d54116b437204cf5ac0fb5d3c58261fa",
            "8e9175f6bcd54fc0a38b5c7f9858ba07",
            "fae39677109247d5b27b7230833751bc",
            "eba39000eb17450185c56d7a74ea9042",
            "99cba555e41a46ff98fd261eedfc6d1d",
            "24aaaa6cc12746e8948e829b8c5cfcf1",
            "f542c7e6c3f24ee3a2952c93d82615b6",
            "3fb7ced1efe144188fc81f99064881dc",
            "e6ea20db0fa7452981fc91054b511e88",
            "72b29df931b145ba871c8c2b737caced",
            "331cc8ba125c434983440e507283bc65",
            "8f6b7c72ad4b443bb1fb07140a99f628",
            "dc9d7bc91be749deb909cca3ce15e907",
            "758da2f475a94fd189a7140017a0e39d",
            "6ac244a8e2bf452b846ac16b77982639",
            "e336a185505c49a29f3a881b536e6c61",
            "473b9b1cbbb44155bdc3047df27cafbd",
            "2280a992a58e427a9dc89a1199125988",
            "76c9beec4a8442df861987b22b65a650",
            "80fe59d20dcd41a39ad0d3a1db237072",
            "8cb84dbcc8c648acb0393bb3a3a78308",
            "6fdbbacc90b74bd7aed1c6d5b369c50a",
            "e322c7c75cc1436d9ac3c189c37f45ff",
            "32664674da114293a2048dc2a4e8fcc7",
            "6ebfbd9e93144d2880d9b991f500812a",
            "ccd1836ebc2c4714b9832f774febd756",
            "3d076721e6f944bdaa8f08e8a6024ed1",
            "5d924653083c4740b3780997bedcf3b0",
            "8aab41c08e824157b68d3558072f8fee",
            "2a65341ca73c42909f97b5b4c0a62783",
            "4f36c6a157ab45b5a65467778b3079eb",
            "e68b08af87be49b2a4ba23f526494cf2",
            "fceea7fa7f7f4cc18a1cd28c8d4722b3",
            "d53775b7db444c27afae1408dfe5eede",
            "69a8acc1e8f745c9831e8ffb5fc71fb6",
            "f930c0d3d8ab41b5b5f3bbe4bcc11475",
            "8f74b57000f540e5911ef20dd6fadde4",
            "e0c22242b06642ba9f724f3ac64be9e4",
            "8728c42806264686a52f1914706a07e8",
            "02eb62ccab894b73bd82b44a90af0e2c",
            "9cd351152c6f44278192e31218e96d25",
            "4a5d110e6ac1413e8840ef5ff1fbb1c0",
            "e1a6db74c21b4fc2ae4dcedf7af2fa9b",
            "56478d32e2e94a63a0fea4e650d7e3b8",
            "9e7fe3b871b74a19bc25be8afa1d7ec5",
            "35dda990a26e40ac8f3c71676f3fdcfa",
            "fb55d38992244c1bbe9f461cf6c0f5d9",
            "9066c7adca884140b92f2458df464350",
            "aa76e77956cf42f49c9dc55fc575de0c",
            "7969a025e1854c218c213da4a438a31f",
            "904d99da25d6460c98fc329ba4a1907c",
            "8b7f1e13a0d840dda16a586173d92299",
            "9e5225501d4f44b69bad566374e1da7e",
            "9741f07c44ad447aaf9d5227990e6da9",
            "414516fd269641db90daf5eacbf47245",
            "fbb925bcb9ca4c5a91c4374b310d7604",
            "c41006b290f343eabebe34a1a9c206f9",
            "bfdd772672024df9835ed7046d952743",
            "3f04bdcd69994b329825733bb0e486c7",
            "877e3fd35a4e49e19d86b7c9565de462",
            "6617881066304152a7615f94552948c7",
            "57718913dc7a47f2ad4b78608d44d38a",
            "24eb531a9d1a4362bca952cf1a8fcab0",
            "2c52e5a9819746d6965575c7ace62638",
            "fd489b6b19a94c6ca3f6ab03f7dc454c",
            "f3524d6694ca4272925f735c1a68dc88",
            "d0975f87ca80443cb87ce6df5e98f98e",
            "3f30575d7fa44dffaa264aa1be294dcd",
            "ff9922708ad44bf1861c64234df0741e",
            "46fe6c9b412a4dfe90deb58b666df780",
            "a89e49cb019a40ec880241dad9303326",
            "24bdd4ce77084a4f83cdcb836e51d5cb",
            "c536983c2a7c48bd81d212dfeacfb4a9",
            "a6ebe2cc27f94596953a27f55ff1e01c",
            "99e53daaf639499b87299a688ab8922a",
            "99154ce51cac471eb33e5d2182b94030",
            "13b1f5f2d2e8497d9071dcdbf53d58a4",
            "f8e1a73d97274a58835a25071375b7f0",
            "e0f333b1555b449394f24ad76a48677f",
            "478c5de88e8841788c0f3ecfcdb52f44",
            "d86505ad24a142e38543b0f01d9e4ad7",
            "8fd8b83d2fc14de89a6009b6fbd4eb47",
            "2155b5a1a7de4be6be48b7f1045308f0",
            "7de76689d9534298bdbadc1353289408",
            "c7e27d78486f4af3972d8071f1652b08",
            "649e7332544842bdbc4d57ed69498cb7",
            "3cf0f4d54171465dba69bd9e7d33dde3",
            "74062c2337134ba0b32151b57bad8069",
            "69545db8d22e416fb7b197071df3c8d3",
            "5a8b6a79c71d475f894935bcf48dba3e",
            "7743ec7a6d9749658087f364791b9a13",
            "58e6cf4842e44424afd42c62adf7cdce",
            "da30e1dda2444f1ca38f373f9f6fa216",
            "23b56c7458d44bf58f1199e7f7d72966",
            "4ff80746ecf245eebe5fdb969095c9fc",
            "5a064ce99e4643d9be5c8df8abaaa38a",
            "8c45223ea56b4200a96686ad402fa695",
            "1167e88826224a83ac58a2e3d2751950",
            "c4ba6bf7c3bb4bd8956ca47491051cda",
            "64dc6c76db4148ada1a26bbcde7fa5d0",
            "52ae01b261a94e98835269f458c76a9f"
          ]
        },
        "id": "unGBkBbyuaPp",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763507897997,
          "user_tz": -60,
          "elapsed": 13888,
          "user": {
            "displayName": "Ayhem Chaabane",
            "userId": "02755421382646347941"
          }
        },
        "outputId": "3376fd82-9d75-47e3-87c0-19b7caaa0a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83c\udfaf CHOISISSEZ VOTRE MOD\u00c8LE:\n",
            "\n",
            "1\ufe0f\u20e3  BlenderBot - Le PLUS POLI et intelligent (RECOMMAND\u00c9) \u2b50\u2b50\u2b50\u2b50\u2b50\n",
            "2\ufe0f\u20e3  DialoGPT Am\u00e9lior\u00e9 - Avec filtres de politesse \u2b50\u2b50\u2b50\n",
            "3\ufe0f\u20e3  GPT-2 Medium - Intelligent mais besoin de prompts \u2b50\u2b50\u2b50\u2b50\n",
            "\n",
            "\ud83d\udce5 Chargement de BlenderBot (plus intelligent et poli)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b90af2e034e46ab8420d9b9f767d35b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fae39677109247d5b27b7230833751bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "758da2f475a94fd189a7140017a0e39d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ebfbd9e93144d2880d9b991f500812a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f930c0d3d8ab41b5b5f3bbe4bcc11475"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb55d38992244c1bbe9f461cf6c0f5d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfdd772672024df9835ed7046d952743"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/730M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff9922708ad44bf1861c64234df0741e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/730M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "478c5de88e8841788c0f3ecfcdb52f44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7743ec7a6d9749658087f364791b9a13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 BlenderBot charg\u00e9 en 13.89s!\n",
            "\n",
            "\u2705 BlenderBot activ\u00e9 - Mode POLI et INTELLIGENT!\n",
            "\n",
            "============================================================\n",
            "\ud83d\udcac Chatbot pr\u00eat - Il sera beaucoup plus poli maintenant!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def discuter(message):\n",
        "    \"\"\"Fonction de discussion simple\"\"\"\n",
        "    if not message.strip():\n",
        "        print(\"\u26a0\ufe0f Message vide\")\n",
        "        return\n",
        "\n",
        "    print(f\"\ud83e\uddd1 Vous: {message}\")\n",
        "\n",
        "    try:\n",
        "        start = time.time()\n",
        "        reponse = bot.generer_reponse(message)\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        print(f\"\ud83e\udd16 Bot: {reponse}\")\n",
        "        print(f\"\u23f1\ufe0f Temps: {elapsed:.2f}s\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Erreur: {e}\\n\")"
      ],
      "metadata": {
        "id": "WTxL-mKfujWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\ud83e\uddea TESTS DE POLITESSE:\\n\")\n",
        "\n",
        "questions_test = [\n",
        "    \"Hello! How are you today?\",\n",
        "    \"What's your favorite color?\",\n",
        "    \"Can you help me with something?\",\n",
        "    \"Tell me about yourself\",\n",
        "    \"What do you think about AI?\"\n",
        "]\n",
        "\n",
        "for q in questions_test:\n",
        "    discuter(q)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2705 Vous voyez la diff\u00e9rence? Le bot est beaucoup plus poli!\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "nqQZvTMIuqI8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763507910830,
          "user_tz": -60,
          "elapsed": 6794,
          "user": {
            "displayName": "Ayhem Chaabane",
            "userId": "02755421382646347941"
          }
        },
        "outputId": "a3377861-cda2-4f53-992b-38bd0f685a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\uddea TESTS DE POLITESSE:\n",
            "\n",
            "\ud83e\uddd1 Vous: Hello! How are you today?\n",
            "\ud83e\udd16 Bot:  I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\n",
            "\u23f1\ufe0f Temps: 2.71s\n",
            "\n",
            "\ud83e\uddd1 Vous: What's your favorite color?\n",
            "\ud83e\udd16 Bot:  My favorite color is blue.  What is yours?  Do you like the color blue?\n",
            "\u23f1\ufe0f Temps: 0.37s\n",
            "\n",
            "\ud83e\uddd1 Vous: Can you help me with something?\n",
            "\ud83e\udd16 Bot:  Sure, what do you need help with?  I'm always willing to lend a hand.\n",
            "\u23f1\ufe0f Temps: 0.37s\n",
            "\n",
            "\ud83e\uddd1 Vous: Tell me about yourself\n",
            "\ud83e\udd16 Bot:  I'm a college student, and I work part time at a grocery store. How about you?\n",
            "\u23f1\ufe0f Temps: 0.41s\n",
            "\n",
            "\ud83e\uddd1 Vous: What do you think about AI?\n",
            "\ud83e\udd16 Bot:  I think it's a great idea. I think we'll have a lot of people working on it.\n",
            "\u23f1\ufe0f Temps: 0.44s\n",
            "\n",
            "\n",
            "============================================================\n",
            "\u2705 Vous voyez la diff\u00e9rence? Le bot est beaucoup plus poli!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conversation_interactive():\n",
        "    \"\"\"Mode conversation continue\"\"\"\n",
        "    print(\"\\n\ud83d\udcac MODE CONVERSATION INTERACTIVE\")\n",
        "    print(\"Tapez 'quit' pour quitter, 'reset' pour r\u00e9initialiser\\n\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            message = input(\"\ud83e\uddd1 Vous: \").strip()\n",
        "\n",
        "            if not message:\n",
        "                continue\n",
        "\n",
        "            if message.lower() in ['quit', 'exit']:\n",
        "                print(\"\ud83d\udc4b Au revoir!\")\n",
        "                break\n",
        "\n",
        "            if message.lower() == 'reset':\n",
        "                if hasattr(bot, 'reinitialiser'):\n",
        "                    bot.reinitialiser()\n",
        "                print(\"\u2705 Conversation r\u00e9initialis\u00e9e!\\n\")\n",
        "                continue\n",
        "\n",
        "            reponse = bot.generer_reponse(message)\n",
        "            print(f\"\ud83e\udd16 Bot: {reponse}\\n\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\ud83d\udc4b Conversation interrompue!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Erreur: {e}\\n\")\n",
        "\n",
        "print(\"\ud83d\udccc Utilisez discuter('votre message') pour tester\")\n",
        "print(\"\ud83d\udccc Ou ex\u00e9cutez conversation_interactive() pour discuter en continu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUl5-Nh6uuRo",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763507914374,
          "user_tz": -60,
          "elapsed": 15,
          "user": {
            "displayName": "Ayhem Chaabane",
            "userId": "02755421382646347941"
          }
        },
        "outputId": "2c2b49ef-b254-4b18-cc99-fafe4f6d490f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udccc Utilisez discuter('votre message') pour tester\n",
            "\ud83d\udccc Ou ex\u00e9cutez conversation_interactive() pour discuter en continu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_interactive()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi8K0CavvQIz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763507926057,
          "user_tz": -60,
          "elapsed": 5060,
          "user": {
            "displayName": "Ayhem Chaabane",
            "userId": "02755421382646347941"
          }
        },
        "outputId": "b6acacd2-e493-4297-a15c-52bd91e93ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcac MODE CONVERSATION INTERACTIVE\n",
            "Tapez 'quit' pour quitter, 'reset' pour r\u00e9initialiser\n",
            "\n",
            "\ud83e\uddd1 Vous: quit\n",
            "\ud83d\udc4b Au revoir!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Trouver automatiquement TON notebook actuel\n",
        "notebooks = [f for f in os.listdir('/content') if f.endswith('.ipynb')]\n",
        "print(\"Notebooks trouv\u00e9s :\", notebooks)\n"
      ],
      "metadata": {
        "id": "jt5qnqMAxcPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763508484053,
          "user_tz": -60,
          "elapsed": 7,
          "user": {
            "displayName": "Ayhem Chaabane",
            "userId": "02755421382646347941"
          }
        },
        "outputId": "33ab1eb6-7440-4ee5-b3e1-c7484162ccae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebooks trouv\u00e9s : []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MdRdy6wmSkAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}